---
title: "Ensembled_clustering"
author: "Chenkai Zhao"
date: "2021/12/13"
output: html_document
---

```{r setup, include=FALSE}
#theme_set(theme_bw()) # change the default ggplot theme to black-and-white

knitr::opts_chunk$set(
  echo=T, ## show your R code chunk
  message = F, ## hide the message
  warning = F, ## hide the warning
  autodep = T ## make sure your separate code chunks can find the dependencies (from other code chunk)
)
```

0.Library all needed packages and connect to the python code files.
```{r}
library(dplyr)
library(tidytext)
library(ROCR) # for plotting roc
library(e1071) # for NB and SVM
library(rpart) # for decision tree
library(ada) # for adaboost
library(text2vec)
library(tidyr)
library(tm)
library(lsa)
library(data.table)
library(magrittr)
library(textstem)
library(stringr)

library(reticulate)
library(aricode)
library(DPBBM)

source_python("ensemble_based_similarity_clustering.py")
source_python("single_pass_incremental_clustering.py")
```

1.Import the data and assign names to the columns.
```{r}
data_test <- read.table("upcoming-wsdm10.txt", head=FALSE, comment.char = "", quote = "", sep ="\t", fill=T, encoding = "UTF-8")

colnames(data_test)<-c("author","title","description","takenDate","postedDate",
                       "tags","location","geoData","eventID")

dim(data_test)
```

2.Split the data into three parts: train, validation and test.
```{r}
split_dataset <- function(data,train_num,valid_num,test_num)
{
  data$id = 1:nrow(data)
  setDT(data)
  setkey(data,id)
  set.seed(2017L)
  all_ids = data$id
  train_ids = all_ids[c(1:train_num)]
  valid_ids = all_ids[c(train_num + 1 : train_num + valid_num)]
  test_ids =  all_ids[c(train_num + valid_num + 1 :nrow(data))]
  
  train = data[J(train_ids)]
  valid = data[J(valid_ids)]
  test = data[J(test_ids)] 
  return(list('train' = train,'valid' = valid,'test' = test))
}

data_splited = split_dataset(data_test,5000,5000,5000)
train = data_splited$train
validation = data_splited$valid
test = data_splited$test
dim(train)
```

3.Implement TFIDF algorithm and apply it to the text features in the dataset and seperate the dataset with different features.
```{r}
TFIDF <- function(textal,id){
  
  prep_fun = function(x) {
    # make text lower case
    x = str_to_lower(x)
    # remove non-alphanumeric symbols
    x = str_replace_all(x, "[^[:alpha:]]", " ")
    # collapse multiple spaces
    x = str_replace_all(x, "\\s+", " ")
    #stemming
    x = stem_strings(x)
  }
  
  tok_fun = word_tokenizer
  
  it_train = itoken(textal, 
                    preprocessor = prep_fun, 
                    tokenizer = tok_fun, 
                    ids = id, 
                    progressbar = FALSE)
  
  stopwords <- c(tm::stopwords("english"))
  
  vocab = create_vocabulary(it_train, stopwords = stopwords)
  vocab = create_vocabulary(it_train)
  
  vectorizer = vocab_vectorizer(vocab)
  t1 = Sys.time()
  dtm_train = create_dtm(it_train, vectorizer)
  print(difftime(Sys.time(), t1, units = 'sec'))
  
  
  tfidf = TfIdf$new()
  # fit model to train data and transform train data with fitted model
  dtm_train_tfidf = fit_transform(dtm_train, tfidf)
  return(as.matrix(dtm_train_tfidf)) 
}

# Get text features from dataset.
title.train.tdidf = TFIDF(train$title, train$id)
des.train.tdidf = TFIDF(train$description, train$id)
tags.train.tdidf = TFIDF(train$tags, train$id)

#Get time feature from dataset and initialize it.
v4 <- train$takenDate
for (i in c(1:length(v4))) {
  v4[i] = unclass(as.POSIXct(v4[i]))/60
}
v4 = as.numeric(v4)
# Get location feature from dataset.
v8 <- train$geoData
```

4.Implement ensemble-based clustering with combining individual similarities and evaluate its performance.
```{r}
#source_python("final.py")
clusters_centroidEnsemble <- single_pass_cluster_centorid_ensemble(5000, tags.train.tdidf, title.train.tdidf, des.train.tdidf, v4, v8, 0.3,c(0.25,0.3,0.95,0.95,0.95), c(0.333,0.197,0.177,0.075,0.218))

nmi_cal <- function(clusters, classi) {
  return(NMI(clusters, classi))
}
nmi <- nmi_cal(clusters_centroidEnsemble, train$eventID)
nmi

bcubed <- BCubed_metric(train$eventID,clusters_centroidEnsemble, 0.5)
bcubed
```

5.Implement ensemble-based clustering with combining individual partitions and evaluate its performance.
```{r}
#Get clustering results of clustering with each features.
clusters_des <- fit_data_text(des.train.tdidf, 0.95)
clusters_title <- fit_data_text(title.train.tdidf, 0.3)
clusters_tags <- fit_data_text(tags.train.tdidf, 0.25)
clusters_geo <- fit_data_location(v8, 0.95)
clusters_time <- fit_data_time(v4, 0.95)

#Calculate the weight of each single-feature clustering result based on their highest NMI and B-cubed.
weight_list <- c(0.197, 0.177, 0.333, 0.075, 0.218)

clusters <- ensembledClustering_partition(clusters_title, clusters_des, clusters_tags, clusters_time, clusters_geo, weight_list, 0.7)

nmi <- nmi_cal(clusters, train$eventID)
nmi

bcubed <- BCubed_metric(train$eventID,clusters, 0.5)
bcubed
```





